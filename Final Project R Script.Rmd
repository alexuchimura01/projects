---
title: "Final Project"
author: "Alex Uchimura"
date: "2024-11-12"
output: html_document
---

What is the ideal tech sales rep? What are the best explanatory variables to predict feedback and NPS scores?

1.  Using feedback as a response

```{r}
library(readxl)
myData <- read_excel("C:/Users/alexa/OneDrive - Cal Poly/Data Mining/Big_Data_Files.xlsx", sheet = "TechSales_Reps")

# making dummy variables for the necessary variables
myData$sentinel <- ifelse(myData$Personality == "Sentinel",1,0)
myData$analyst <- ifelse(myData$Personality == "Analyst",1,0)
myData$diplomat <- ifelse(myData$Personality == "Diplomat",1,0)
myData$explorer <- ifelse(myData$Personality == "Explorer",1,0)

myData$hardware <- ifelse(myData$Business == "Hardware",1,0)
myData$software <- ifelse(myData$Business == "Software",1,0)

myData$College <- ifelse(myData$College == "Yes",1,0)

myData

# using feedback as response SHOULD I STANDARDIZE?
feedback_fullModel <- lm(Feedback ~ Age + Female + Years + College + analyst + diplomat + explorer + Certficates + software, myData)
summary(feedback_fullModel)
# adj R2 = 0.000006163

feedback_interModel1 <- lm(Feedback ~ Age + Female + Age*Years + College + analyst + diplomat + explorer + Certficates + software, myData)
summary(feedback_interModel)
# adj R2 = 0.000004023


feedback_Model2 <- lm(Feedback ~ Age + Female + Years + College + Certficates + software, myData)
summary(feedback_Model2)
# adj R2 = 0.000168

feedback_Model3 <- lm(Feedback ~ Age + Female + College + analyst + diplomat + explorer + Certficates + software, myData)
summary(feedback_Model3)
# adj R2 = 0.000107

feedback_Model4 <- lm(Feedback ~ Age + Female + Years + College + analyst + diplomat + explorer + Certficates, myData)
summary(feedback_Model4)
# adj R2 = -0.0001344

plot(feedback_model)


```

2.  Using NPS as a response

```{r}
# using NPS as response
nps_fullModel <- lm(NPS ~ Age + Female + Years + College + analyst + diplomat + explorer + Certficates + software, myData)
summary(nps_fullModel)
# adj R2 = 0.4094

nps_interModel1 <- lm(NPS ~ Age + Female + Years + College + analyst + diplomat + explorer + Certficates + software, myData)
summary(nps_interModel1)
# adj R2 = 0.4094
```

Do:

1.  A k-means cluster analysis w/ all numerical variables and see which k has the best clusters with the highest silhouette widths

#### K-Means Cluster Analysis (creating a representative sample of 500)

```{r}
library(cluster)
suppressWarnings(RNGversion("3.5.3"))
set.seed(1)
kResult <- pam(num_sample_std, k=3)
summary(kResult)
plot(kResult)

```

1.  A agglomerative hierarchical cluster analysis to find a good value of k clusters for the k-means analysis (only with numerical variables, maybe do that one that you split the dataset into subsets of different levels of binary variables (female dataset, non-female dataset, software, hardware, etc.)

```{r}
library(cluster)
# using a sample of 3500 observations to do hierarchical clustering

set.seed(123)

# Randomly sample 3,500 rows
sample_data <- myData[sample(1:nrow(myData), 5000, replace = FALSE), ]
sample_data
# Check the sample
head(sample_data)

num_sample <- sample_data[, -c(1, 2, 4, 6, 7)]
num_sample

num_sample_std <- scale(num_sample)

d <- dist(num_sample_std, method="euclidean")

aResult <- agnes(d, diss=TRUE, method="ward")
aResult

plot(aResult)
```

ward agg. coef. = 0.99

Looks like 4 clusters is an ideal amount from this subset of 3500 observations

```{r}
myData
num_myData <- myData[, -c(1, 2, 4, 6, 7, 12, 13, 14, 15, 16, 17)] # excluding Sales_Rep, Business, Female, College, and Personality
num_myData

num_myData_std <- scale(num_myData)

d <- dist(sample_data, method="euclidean")

aResult <- agnes(d, diss=TRUE, method="single")
aResult
```

1.  

2.  A agglomerative cluster analysis with the mixed data using the daisy function

```{r}
library(cluster)
suppressWarnings(RNGversion("3.5.3"))
set.seed(1)

sample_data <- [sample(1:nrow(myData), 5000, replace = FALSE), ]

sample_data
sample_data[, c(2, 4, 6, 7)] <- lapply(sample_data[, c(2, 4, 6, 7)], as.factor)

d <- daisy(sample_data[ , 2:11], metric = "gower")

mResult <- agnes(d, method = "ward")
mResult

plot(mResult)

mClusters <- cutree(mResult, k=4)

sample_data <- data.frame(sample_data, mClusters)
summary(as.factor(mClusters))


```

1.  Decide on a final best cluster analysis btwn steps 2 and 3
2.  Try to argue that each cluster has a different type of person that is more deserving of a higher, medium, or lower salary

Principal Component Analysis Work

```{r}
myData
# making dummy variables for the necessary variables
myData$sentinel <- ifelse(myData$Personality == "Sentinel",1,0)
myData$analyst <- ifelse(myData$Personality == "Analyst",1,0)
myData$diplomat <- ifelse(myData$Personality == "Diplomat",1,0)
myData$explorer <- ifelse(myData$Personality == "Explorer",1,0)

myData$hardware <- ifelse(myData$Business == "Hardware",1,0)
myData$software <- ifelse(myData$Business == "Software",1,0)
myData$College <- ifelse(myData$College == "Yes",1,0)

all_num_preds <- myData[, -c(1, 2, 7, 9, 11)]
all_num_preds

scaled_numeric <- scale(all_num_preds[, c(1, 3, 5, 6)])
binary <- all_num_preds[, c(2, 4, 7, 8, 9, 10, 11, 12)]

st.preds <- cbind(binary, scaled_numeric)

pca <- prcomp(st.preds)
summary(pca)

pca$rotation

pca$x

newData <- data.frame(myData, pca$x)
newData

pc_feedback_Data <- newData[, c(9, 18, 19, 20, 21, 22, 23, 24, 25)]
pc_feedback_Data
pc_nps_Data <- newData[, c(11, 18, 19, 20, 21, 22, 23, 24, 25)]
```

```{r}
loadings <- pca$rotation

# Find the top contributing variables for each PC
important_vars <- apply(loadings, 2, function(x) names(sort(abs(x), decreasing = TRUE)[1:3]))
important_vars
```

```{r}
feedback_model <- lm(Feedback ~ PC1 + PC2 + PC3 + PC4 + PC5 + PC6 + PC7 + PC8, pc_feedback_Data)
summary(feedback_model)
```

```{r}
nps_model <- lm(NPS ~ PC1 + PC2 + PC3 + PC4 + PC5 + PC6 + PC7 + PC8, pc_nps_Data)
summary(nps_model)
```

```{r}
# agglomerative clustering for a random sample of n=6000 from all numerical variables


sample_data <- all_num_data[sample(1:nrow(all_num_data), 10000, replace = FALSE), ]
sample_data

# K-means clustering for a random sample of n=10000 from all numerical variables
library(cluster)
suppressWarnings(RNGversion("3.5.3"))
set.seed(1)
kResult <- pam(sample_data, k=3)
summary(kResult)
plot(kResult)
```

